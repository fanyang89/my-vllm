# https://taskfile.dev

version: "3"

tasks:
  vllm:
    env:
      LMCACHE_CONFIG_FILE: lmcache.yml
    cmds:
      - >
        uv run vllm serve "Qwen/Qwen3-4B"
        --max-model-len 32768
        --kv-transfer-config '{"kv_connector": "LMCacheConnectorV1", "kv_role": "kv_both"}'

  lmcache:
    cmds:
      - uv run lmcache_server 127.0.0.1 65432

  test:
    cmds:
      - >
        curl http://localhost:8000/v1/completions
        -H "Content-Type: application/json"
        -d '{
          "model": "Qwen/Qwen3-4B",
          "prompt": "<|begin_of_text|><|system|>\nYou are a helpful AI assistant.\n<|user|>\nWhat is the capital of France?\n<|assistant|>",
          "max_tokens": 100,
          "temperature": 0.7
        }'

  print:
    cmds:
      - uv run python -c "import torch; print(torch.__version__); print(torch.cuda.get_arch_list()); print(torch.randn(1).cuda())"

  cuda:
    cmds:
      - uv run python -c "import torch; print(torch.cuda.is_available()); print(torch.randn(1).cuda())"
