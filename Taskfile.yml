# https://taskfile.dev

version: "3"

tasks:
  serve:
    env:
      LMCACHE_CONFIG_FILE: lmcache.yml
    cmds:
      - >
        vllm serve "Qwen/Qwen3-4B"
        --max-model-len 32768
        --kv-transfer-config '{"kv_connector": "LMCacheConnectorV1", "kv_role": "kv_both"}'

  test:
    cmds:
      - >
        curl http://localhost:8000/v1/completions
        -H "Content-Type: application/json"
        -d '{
          "model": "Qwen/Qwen3-4B",
          "prompt": "<|begin_of_text|><|system|>\nYou are a helpful AI assistant.\n<|user|>\nWhat is the capital of France?\n<|assistant|>",
          "max_tokens": 100,
          "temperature": 0.7
        }'

  print:
    cmds:
      - uv run python -c "import torch; print(torch.__version__); print(torch.cuda.get_arch_list()); print(torch.randn(1).cuda())"
