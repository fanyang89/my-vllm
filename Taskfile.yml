version: "3"

vars:
  IMAGE_NAME: docker.io/fuis/tritonserver:25.05-vllm-python-py3-lmcache

tasks:
  vllm:
    cmds:
      # with hf mirror
      # -e HF_ENDPOINT=https://hf-mirror.com
      - >
        docker run --rm --gpus all --ipc=host --net=host
        --ulimit memlock=-1 --ulimit stack=67108864
        -e CUDA_VISIBLE_DEVICES=0
        -e LMCACHE_CONFIG_FILE=/config/lmcache/lmcache.yml
        -e LMCACHE_USE_EXPERIMENTAL=True
        -v /home/fanmi/.cache:/root/.cache
        -v $(pwd)/config/lmcache:/config/lmcache
        {{.IMAGE_NAME}}
        vllm serve
        Qwen/Qwen3-0.6B
        --max-model-len=8192
        --kv-transfer-config '{"kv_connector": "LMCacheConnector", "kv_role": "kv_both"}'

  lmcache:
    cmds:
      - uv run lmcache_server 127.0.0.1 65432

  test:
    cmds:
      - >
        curl "http://127.0.0.1:8000/v1/completions"
        -H 'Content-Type: application/json'
        -d '{"prompt": "How are you?"}'

  print:
    vars:
      TEST_CMD: >
                import torch;
                print(torch.__version__);
                print(torch.cuda.get_arch_list());
                print(torch.randn(1).cuda());
                import vllm.distributed.kv_transfer.kv_connector.lmcache_connector;
    cmds:
      - >
        docker run --rm --gpus all --ipc=host --net=host
        --ulimit memlock=-1 --ulimit stack=67108864
        -e CUDA_VISIBLE_DEVICES=0
        {{.IMAGE_NAME}}
        python3 -c "{{.TEST_CMD}}"

  docker:
    cmds:
      - docker buildx build -t {{.IMAGE_NAME}} .
